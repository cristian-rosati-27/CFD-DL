{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import knn_graph\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "\n",
    "import gc\n",
    "\n",
    "from segnn.segnn import SEGNN\n",
    "from e3nn.o3 import Irreps, spherical_harmonics\n",
    "from segnn.balanced_irreps import BalancedIrreps, WeightBalancedIrreps\n",
    "from segnn.instance_norm import InstanceNorm\n",
    "# from Utility_functions import Graph_datasetV2 as Graph_dataset\n",
    "\n",
    "# use it for input features similar to hemodynamics paper\n",
    "from Utility_functions import Graph_dataset_with_equiv_features, inlet_distance_mask, print_3D_graph, Graph_dataset_with_equiv_features\n",
    "\n",
    "import params\n",
    "\n",
    "print('DATADIR', params.DATADIR)\n",
    "print('NSIM', params.NSIM)\n",
    "print('BATCH_SIZE', params.BATCH_SIZE)\n",
    "\n",
    "print('python.__version__', sys.version_info)\n",
    "print('torch.__version__', torch.__version__)\n",
    "print('torch_geometric.__version__', torch_geometric.__version__)\n",
    "print('torch.cuda.is_available()', torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Irreps('1o+2e+3x0e')\n",
    "b = Irreps('4x1o+2o+2x1e')\n",
    "\n",
    "c = Irreps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PyTorch has version {}\".format(torch.__version__))\n",
    "print(\"The linked CUDA version is\", torch.version.cuda)\n",
    "\n",
    "dev = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# dev = torch.device(\"cpu\")\n",
    "# torch.cuda.init()\n",
    "\n",
    "print('Running on device:', dev)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d\n",
    "\n",
    "for args in [\n",
    "     \n",
    "        {'epochs': params.EPOCHS,\n",
    "         'batch_size': params.BATCH_SIZE,          \n",
    "         'input_size' : params.INPUT_SIZE, \n",
    "         'edge_lmax' : params.EDGE_LMAX,\n",
    "         'node_lmax' : params.NODE_LMAX,\n",
    "         'hidden_lmax' : params.HIDDEN_LMAX,\n",
    "         'num_layers' : params.NUM_LAYERS,\n",
    "         'task' : params.TASK,\n",
    "         'norm' : params.NORM,\n",
    "         'output_size' : params.OUTPUT_SIZE,\n",
    "         'hidden_size': params.HIDDEN_SIZE, \n",
    "         'neighbours' : params.NEIGHBOURS,\n",
    "         'subsample_dataset': params.SUBSAMPLE_DATASET,\n",
    "         'opt': params.OPT,\n",
    "         'scheduler': params.SCHEDULER, \n",
    "         'learning_rate': params.LEARNING_RATE,\n",
    "         'device': params.DEVICE,\n",
    "         'early_stop' : params.EARLY_STOP}\n",
    "    ]:\n",
    "        args = objectview(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset and dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset, change the root path accordingly\n",
    "dataset = Graph_dataset_with_equiv_features(root = params.DATADIR)  #, inlet_mask = True)\n",
    "test_sample = dataset[0]\n",
    "print(test_sample.edge_index)\n",
    "\n",
    "# 80-10-10 split\n",
    "\n",
    "# first the dataset is split 80%-20%, with the first portion being the training set\n",
    "\n",
    "if params.NSIM == 1:\n",
    "    train_idx = list(range(1))\n",
    "    val_test_idx = []\n",
    "elif params.NSIM == 10:  # in this case the test_size cannot be smaller than 2\n",
    "    train_idx, val_test_idx = train_test_split(range(len(dataset)), test_size = 0.2, shuffle = False)\n",
    "else:\n",
    "    train_idx, val_test_idx = train_test_split(range(len(dataset)), test_size = 0.1, shuffle = False)\n",
    "\n",
    "print(train_idx)\n",
    "print(val_test_idx)\n",
    "# the 20% portion is split in half so to have two sets with 10% data each of the original dataset\n",
    "if params.NSIM == 1:\n",
    "    val_idx, test_idx = [], []\n",
    "else:\n",
    "    val_idx, test_idx = train_test_split(range(len(val_test_idx)), test_size = 0.5, shuffle = False)\n",
    "\n",
    "# this line can be used to do training on a smaller subset of the training set\n",
    "# if args.subsample_dataset != 1\n",
    "train_idx = train_idx[:(len(train_idx))// args.subsample_dataset] \n",
    "\n",
    "train_dataset = dataset[train_idx]\n",
    "val_dataset = dataset[val_idx]\n",
    "\n",
    "# print(len(train_idx), len(val_idx), len(test_idx))\n",
    "# print(len(train_idx) / len(dataset), len(val_idx) / len(dataset), len(test_idx) / len(dataset))\n",
    "\n",
    "# torch_geometric DataLoaders are used for handling lists of graphs\n",
    "n_works = 0\n",
    "t_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=n_works)#, persistent_workers=True)\n",
    "v_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=n_works)#, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test if graph is loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = torch.tensor(np.load(params.DATADIR+\"/connectivity.npy\"))\n",
    "print_3D_graph(test_sample.pos.cpu(), edges = edges, color = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'SEGNN_model'\n",
    "project = \"local_SEGNN_with_inlet_trials\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for sample in t_loader:\n",
    "    break\n",
    "\n",
    "print(t_loader.dataset[0].node_attr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(sample)\n",
    "print(len(sample.node_attr[:]))\n",
    "\n",
    "# checking number of zero entries on node attributes; they are a lot!\n",
    "print((torch.sum(sample.node_attr,dim=-1)==0).sum())\n",
    "\n",
    "print_3D_graph(sample[0].pos.cpu(), edges = edges, color = None)\n",
    "print(sample[0].node_attr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print()\n",
    "print(torch.cuda.memory_allocated()*4/(1024**2), \"MB\")\n",
    "print(torch.cuda.max_memory_allocated()*4/(1024**2), \"MB\")\n",
    "print()\n",
    "# print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#input_irreps=Irreps('5x0e'), # x: node features [SDF,MIS,3x0e one_hot encodng]\n",
    "\n",
    "\n",
    "# x: node features for fluid nodes [distance vector to closest inlet node,\n",
    "#                                   distance vector to closest outlet node,\n",
    "#                                   SDF (rel_dist to wall)]\n",
    "# 2x1o + 0e means 2 vectors (odd parity) and one scalar (even parity)\n",
    "input_irreps = Irreps('2x1o + 2x0e')\n",
    "\n",
    "# node_attr_irreps=Irreps.spherical_harmonics(lmax=args.node_lmax)\n",
    "node_attr_irreps = Irreps('0e+1o')\n",
    "\n",
    "\n",
    "# black box to specify the hidden layer, with some parametrization\n",
    "hidden_irreps = BalancedIrreps(lmax=args.hidden_lmax, vec_dim=args.hidden_size)\n",
    "\n",
    "# y: node features to predict [P,vx,vy,vz]\n",
    "output_irreps = Irreps('0e + 1o')\n",
    "\n",
    "# edge features (relative distance vector and norm)\n",
    "edge_attr_irreps = Irreps.spherical_harmonics(lmax=args.edge_lmax)\n",
    "\n",
    "# no additional attributes specified\n",
    "additional_message_irreps = None # Irreps('1x0e')\n",
    "\n",
    "model = SEGNN(hidden_irreps=hidden_irreps,\n",
    "              output_irreps=output_irreps,\n",
    "              edge_attr_irreps=edge_attr_irreps,\n",
    "              node_attr_irreps=node_attr_irreps,\n",
    "              input_irreps=input_irreps,\n",
    "              task=args.task,\n",
    "              norm=args.norm,\n",
    "              num_layers=args.num_layers,\n",
    "              additional_message_irreps=additional_message_irreps\n",
    "              )\n",
    "\n",
    "model.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# num_input_features = sample.x.shape[1]  # 8 if x is [60, 8]\n",
    "# print(f\"x shape: {num_input_features}\")\n",
    "# num_output_features = sample.y.shape[1]  # 4 if y is [60, 4]\n",
    "# print(f\"y shape: {num_output_features}\")\n",
    "# num_node_attr = sample.node_attr.shape[1]  # 4 if node_attr is [60, 4]\n",
    "# print(f\"node_attr shape: {num_node_attr}\")\n",
    "\n",
    "# # assest dimensions automatically from input data dimensions\n",
    "# if num_input_features == 8:\n",
    "#     # x: node features for fluid nodes [distance vector to closest inlet node,\n",
    "#     #                                   distance vector to closest outlet node,\n",
    "#     #                                   SDF (rel_dist to wall)]\n",
    "#     # 2x1o + 0e means 2 vectors (odd parity) and one scalar (even parity)\n",
    "#     input_irreps = Irreps('2x1o + 2x0e') \n",
    "# elif num_input_features == 6:\n",
    "#     input_irreps = Irreps('2x1o')\n",
    "# else:\n",
    "#     raise ValueError(f\"Unexpected dimensions for input features tensor: {num_input_features}\")\n",
    "\n",
    "# # y: node features to predict [MIS,vx,vy,vz]\n",
    "# if num_output_features == 4:\n",
    "#     output_irreps = Irreps('0e + 1o')\n",
    "# else:\n",
    "#     raise ValueError(f\"Unexpected dimensions for output features tensor: {num_output_features}\")\n",
    "\n",
    "# if num_node_attr == 4:\n",
    "#     # node_attr_irreps=Irreps.spherical_harmonics(lmax=args.node_lmax)\n",
    "#     node_attr_irreps = Irreps('0e + 1o') \n",
    "# else:\n",
    "#     raise ValueError(f\"Unexpected dimensions for node attr tensor: {num_node_attr}\")\n",
    "\n",
    "\n",
    "# # black box to specify the hidden layer, with some parametrization\n",
    "# hidden_irreps = BalancedIrreps(lmax=args.hidden_lmax, vec_dim=args.hidden_size)\n",
    "\n",
    "\n",
    "# # edge features (relative distance vector and norm)\n",
    "# edge_attr_irreps = Irreps.spherical_harmonics(lmax=args.edge_lmax)\n",
    "\n",
    "# # no additional attributes specified\n",
    "# additional_message_irreps = None # Irreps('1x0e')\n",
    "\n",
    "# model = SEGNN(hidden_irreps=hidden_irreps,\n",
    "#               output_irreps=output_irreps,\n",
    "#               edge_attr_irreps=edge_attr_irreps,\n",
    "#               node_attr_irreps=node_attr_irreps,\n",
    "#               input_irreps=input_irreps,\n",
    "#               task=args.task,\n",
    "#               norm=args.norm,\n",
    "#               num_layers=args.num_layers,\n",
    "#               additional_message_irreps=additional_message_irreps\n",
    "#               )\n",
    "\n",
    "# model.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "nparams = count_parameters(model)\n",
    "print('Number of parameters; ',nparams)\n",
    "\n",
    "mem_params = sum([param.nelement()*param.element_size() for param in model.parameters()])\n",
    "mem_bufs = sum([buf.nelement()*buf.element_size() for buf in model.buffers()])\n",
    "print()\n",
    "print('Memory (MB) occupied by parameters and buffers: ',mem_bufs, '\\t', mem_params)\n",
    "print('Memory allocated on GPU:', torch.cuda.memory_allocated()*4/(1024**2), \"MB\")\n",
    "print('Peak memory allocated on GPU:', torch.cuda.max_memory_allocated()*4/(1024**2), \"MB\")\n",
    "# print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loss and optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_func = nn.MSELoss()\n",
    "loss_func = nn.L1Loss()\n",
    "if args.opt == 'Adam':\n",
    "    opt = optim.Adam(model.parameters(), lr = args.learning_rate)\n",
    "if args.scheduler == 'ExponentialLR':\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=params.GAMMA)\n",
    "else:\n",
    "    scheduler = None    \n",
    "\n",
    "model_name = name + '_lat'+str(args.hidden_size) + '_knn' + \\\n",
    "            str(args.neighbours) + '_bs' + str(args.batch_size) + '_lr' + str(args.learning_rate) + '_ep' + \\\n",
    "            str(args.epochs) + '_dataset' + str(params.DATASET) + '_nparams' + str(nparams) +'.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_norm = True\n",
    "\n",
    "input_norm = InstanceNorm(input_irreps)\n",
    "edge_norm = InstanceNorm(edge_attr_irreps)\n",
    "# node_norm = InstanceNorm(node_attr_irreps)\n",
    "# output_norm = InstanceNorm(output_irreps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, opt, loss_func, dev, log = True, mask = False):\n",
    "     \n",
    "    model.train()\n",
    "\n",
    "    training_loss = []       \n",
    "\n",
    "    edge_index = torch.tensor(np.load(params.DATADIR+\"/connectivity.npy\"))\n",
    "    \n",
    "    for sample in loader:\n",
    "\n",
    "        #print('SM...sample.x', sample.x.shape)\n",
    "        # edge_index = knn_graph(sample.pos, args.neighbours, sample.batch)\n",
    "        \n",
    "        sample.edge_index = edge_index\n",
    "        \n",
    "        # computes relative positions for every node pair\n",
    "        edge_relativePos = (torch.index_select(sample.pos, 0, edge_index[1]) - torch.index_select(sample.pos, 0, edge_index[0]))\n",
    "        # computes distances between pairs\n",
    "        edge_relativeDist = torch.norm(edge_relativePos, dim = -1, keepdim = True) \n",
    "        # concatenates relative pos and distance for every edge\n",
    "        edge_attr = torch.cat([edge_relativeDist, edge_relativePos], dim = -1) \n",
    "        \n",
    "        sample.edge_attr = edge_attr\n",
    "        # sample.node_attr = sample.pos\n",
    "        \n",
    "        if instance_norm:\n",
    "            sample.x = input_norm(sample.x, sample.batch)\n",
    "            sample.edge_attr = edge_norm(sample.edge_attr, sample.edge_index[1,:])\n",
    "\n",
    "        sample = sample.to(dev)\n",
    "        #print(sample)\n",
    "        # print(sample.x)\n",
    "        # print(sample.pos)\n",
    "        #print(sample.node_attr)\n",
    "        \n",
    "        fluid_nodes = torch.tensor(2)\n",
    "        \n",
    "        if mask:\n",
    "            # sample.mask: node next to inlet\n",
    "            # take mask of nodes far from inlet\n",
    "            loss_mask = ~sample.mask\n",
    "        else:\n",
    "            loss_mask = torch.ones(sample.x.shape[0], dtype=torch.bool)\n",
    "\n",
    "\n",
    "        opt.zero_grad()\n",
    "    \n",
    "        pred = model(sample)      \n",
    "        #print('train', pred[0])# pred.shape, pred[loss_mask].shape, sample.y.shape)\n",
    "        loss = loss_func(pred[loss_mask], sample.y[loss_mask])  \n",
    "\n",
    "        loss.backward()\n",
    "        opt.step() \n",
    "        \n",
    "        training_loss.append(loss)\n",
    "\n",
    "        if log == True:\n",
    "            wandb.log({\"train/batch_train_loss\": loss})\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     magvel = torch.norm(pred[mask,1:4], dim=-1)\n",
    "        #     print('magvel', magvel)\n",
    "        #     print_3D_graph(sample[0].pos, edges = None, color = magvel)\n",
    "        \n",
    "    return sum(training_loss) / len(loader)\n",
    "\n",
    "\n",
    "\n",
    "def val(model, loader, loss_func, mask = False):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "   \n",
    "        validation_loss = [] \n",
    "\n",
    "        edge_index = torch.tensor(np.load(params.DATADIR+\"/connectivity.npy\"))\n",
    "\n",
    "\n",
    "        for sample in loader:\n",
    "            \n",
    "            # edge_index = knn_graph(sample.pos, args.neighbours, sample.batch)\n",
    "            sample.edge_index = edge_index\n",
    "            \n",
    "            edge_relativePos = (torch.index_select(sample.pos, 0, edge_index[1]) - torch.index_select(sample.pos, 0, edge_index[0]))\n",
    "            edge_relativeDist = torch.norm(edge_relativePos, dim = -1, keepdim = True) \n",
    "            edge_attr = torch.cat([edge_relativeDist, edge_relativePos], dim = -1) \n",
    "            \n",
    "            sample.edge_attr = edge_attr\n",
    "            # sample.node_attr = sample.pos\n",
    "            \n",
    "            if instance_norm:\n",
    "                sample.x = input_norm(sample.x, sample.batch)\n",
    "                sample.edge_attr = edge_norm(sample.edge_attr, sample.edge_index[1,:])\n",
    "            \n",
    "            sample = sample.to(dev)\n",
    "            \n",
    "            fluid_nodes = torch.tensor(2)\n",
    "            if mask:\n",
    "                # sample.mask: node next to inlet\n",
    "                # take mask of nodes far from inlet\n",
    "                loss_mask = ~sample.mask\n",
    "            else:\n",
    "                loss_mask = torch.ones(sample.x.shape[0], dtype=torch.bool)          \n",
    "\n",
    "            pred = model(sample)\n",
    "            #print('validation', pred[0])#pred.shape, pred[loss_mask].shape, sample.y.shape)\n",
    "            loss = loss_func(pred[loss_mask], sample.y[loss_mask])  \n",
    "            validation_loss.append(loss)\n",
    "            \n",
    "            ### test stuff\n",
    "            print(f\"sample: {sample.y[0]}\")\n",
    "            print(f\"pred: {pred[0]}\")\n",
    "\n",
    "    return sum(validation_loss) / len(loader)\n",
    "\n",
    "\n",
    "def early_stopping(val_loss, best_loss, counter):\n",
    "        \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "    return counter, best_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training and validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 0\n",
    "best_loss = 10**6\n",
    "counter = 0\n",
    "log = True\n",
    "\n",
    "wandb.init(  \n",
    "      project= project,    \n",
    "      config={\n",
    "        \"epochs\": args.epochs,\n",
    "        \"bs\": args.batch_size,\n",
    "        \"lr\": args.learning_rate,\n",
    "        \"neighbours\": args.neighbours,\n",
    "        \"latent size\": args.hidden_size,\n",
    "        \"model parameters\": nparams\n",
    "        })\n",
    "\n",
    "path = os.path.join(params.DATADIR, 'checkpoints', model_name)\n",
    "\n",
    "wandb.run.name = model_name\n",
    "\n",
    "# mask = True\n",
    "mask = False\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    # training \n",
    "    train_loss = train(model, t_loader, opt, loss_func, dev, log, mask)\n",
    "    #train_loss = train(model, train_dataset, opt, loss_func, dev, log, mask)\n",
    "    if scheduler != None: \n",
    "        scheduler.step()\n",
    "    samples += len(train_dataset)\n",
    "\n",
    "    metrics = {\"train/train_loss\": train_loss,  \n",
    "                \"train/samples\": samples}\n",
    "    wandb.log(metrics)    \n",
    "    \n",
    "    # validation\n",
    "    val_loss = val(model, v_loader, loss_func, mask)\n",
    "    # val_loss = val(model, val_dataset, loss_func, mask)\n",
    "\n",
    "    val_metrics = {\"val/val_loss\": val_loss,\n",
    "                   \"epoch\": epoch+1}\n",
    "\n",
    "    wandb.log(val_metrics)\n",
    "    \n",
    "    # early stopping\n",
    "    counter, best_loss = early_stopping(val_loss, best_loss, counter)\n",
    "    if counter == 0:\n",
    "\n",
    "        checkpoint = {\n",
    "            'epoch': epoch+1,\n",
    "            'model': model,\n",
    "            'optimizer_state_dict': opt.state_dict(),\n",
    "            'training_loss': train_loss,\n",
    "            'validation_loss': val_loss,\n",
    "            'input_irreps':input_irreps,\n",
    "            'hidden_irreps': hidden_irreps,\n",
    "            'output_irreps': output_irreps,\n",
    "            'edge_attr_irreps': edge_attr_irreps,\n",
    "            'node_attr_irreps': node_attr_irreps,\n",
    "            'task': args.task,\n",
    "            'norm': args.norm,\n",
    "            'num_layers': args.num_layers,\n",
    "            'additional_message_irreps': additional_message_irreps\n",
    "        }\n",
    "\n",
    "\n",
    "    if counter >= args.early_stop:        \n",
    "\n",
    "        torch.save(checkpoint, path)\n",
    "        wandb.alert(\n",
    "            title=\"Early stopping on validation data\", \n",
    "            text=f\"Loss {best_loss} was the best result, now is overfitting\")\n",
    "        break\n",
    "\n",
    "    \n",
    "    print(\"Epoch \" + str(epoch+1) + \": T loss \" + str(train_loss) + \" V loss \" + str(val_loss))\n",
    "        \n",
    "torch.save(checkpoint, path)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "import re\n",
    "\n",
    "numbers = re.compile(r'(\\d+)')\n",
    "\n",
    "def numericalSort(value):\n",
    "    parts = numbers.split(value)\n",
    "    parts[1::2] = map(int, parts[1::2])\n",
    "    return parts\n",
    "\n",
    "probes_path = os.path.join(params.DATADIR, 'probes')\n",
    "probes = sorted(glob.glob(os.path.join(probes_path, '*.npy')), key = numericalSort)\n",
    "# print(probes[0])\n",
    "\n",
    "probe_data_list = []\n",
    "\n",
    "for data_file in probes:\n",
    "    read_points = np.load(data_file)\n",
    "    probe_data_list.append(torch.from_numpy(read_points))\n",
    "\n",
    "pos_list = probe_data_list\n",
    "#print(pos_list)\n",
    "# print(pos_list[0][...,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    validation_loss = [] \n",
    "\n",
    "    edge_index = torch.tensor(np.load(params.DATADIR+\"/connectivity.npy\"))\n",
    "\n",
    "    predictions = []\n",
    "    loss_velocity = []\n",
    "    loss_pressure = []\n",
    "    true_mis = []\n",
    "\n",
    "    i = 1\n",
    "    for sample in v_loader:\n",
    "        \n",
    "        # edge_index = knn_graph(sample.pos, args.neighbours, sample.batch)\n",
    "        sample.edge_index = edge_index\n",
    "        \n",
    "        edge_relativePos = (torch.index_select(sample.pos, 0, edge_index[1]) - torch.index_select(sample.pos, 0, edge_index[0]))\n",
    "        edge_relativeDist = torch.norm(edge_relativePos, dim = -1, keepdim = True) \n",
    "        edge_attr = torch.cat([edge_relativeDist, edge_relativePos], dim = -1) \n",
    "        \n",
    "        sample.edge_attr = edge_attr\n",
    "        # sample.node_attr = sample.pos\n",
    "        \n",
    "        if instance_norm:\n",
    "            sample.x = input_norm(sample.x, sample.batch)\n",
    "            sample.edge_attr = edge_norm(sample.edge_attr, sample.edge_index[1,:])\n",
    "        \n",
    "        sample = sample.to(dev)\n",
    "        \n",
    "        fluid_nodes = torch.tensor(2)\n",
    "        if mask:\n",
    "            # sample.mask: node next to inlet\n",
    "            # take mask of nodes far from inlet\n",
    "            loss_mask = ~sample.mask\n",
    "        else:\n",
    "            loss_mask = torch.ones(sample.x.shape[0], dtype=torch.bool)          \n",
    "\n",
    "        pred = model(sample)\n",
    "        # print(sample)\n",
    "        #print('validation', pred[0])#pred.shape, pred[loss_mask].shape, sample.y.shape)\n",
    "        loss = loss_func(pred[loss_mask], sample.y[loss_mask])  \n",
    "        validation_loss.append(loss)\n",
    "\n",
    "        predictions.append(pred)\n",
    "\n",
    "        # loss calculation\n",
    "        true_vel = torch.norm(sample.y[..., 1:4], dim=-1) \n",
    "        # print(len(sample.y))\n",
    "        true_press = sample.y[..., 0]\n",
    "        out_vel = torch.norm(pred[...,1:4], dim=-1).cpu()\n",
    "        out_press = pred[..., 0]\n",
    "        loss_velocity.append(100*abs((true_vel-out_vel)/true_vel))\n",
    "        loss_pressure.append(100*abs((true_press-out_press)/true_press))\n",
    "\n",
    "        # true_mis.append(pos_list[i][...,3])\n",
    "\n",
    "        current_mis = pos_list[i][...,3]\n",
    "        repeated_mis = torch.cat([current_mis, current_mis])\n",
    "        true_mis.append(repeated_mis)\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "print(len(loss_velocity))\n",
    "print(len(loss_pressure))\n",
    "print(len(true_mis))\n",
    "\n",
    "print(len(loss_velocity[0]))\n",
    "print(len(loss_pressure[0]))\n",
    "print(len(true_mis[0]))\n",
    "\n",
    "print(loss_velocity)\n",
    "print(true_mis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# print(len(loss_val))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "loss_velocity_flat = np.concatenate(loss_velocity)\n",
    "true_mis_flat = np.concatenate(true_mis)\n",
    "\n",
    "print(len(true_mis_flat))\n",
    "print(len(loss_velocity_flat))\n",
    "color = plt.cm.hsv(np.linspace(0, 1, len(true_mis_flat)))\n",
    "# color = np.arange(len(loss_velocity))\n",
    "plt.scatter(true_mis_flat, loss_velocity_flat, marker = \".\", c = color, cmap = 'hsv', s = 10)\n",
    "plt.colorbar()\n",
    "\n",
    "# for i in range(len(loss_vel_z)):\n",
    "#     plt.text(true_mis[i], loss_vel_z[i], str(i), fontsize=4, ha='right')\n",
    "\n",
    "plt.title(\"Velocity error on test sample\", size= 15)\n",
    "plt.xlabel(\"MIS\", size=15)\n",
    "plt.ylabel(\"Loss in %\", size=15)\n",
    "# plt.semilogy()\n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "loss_pressure_flat = np.concatenate(loss_pressure)\n",
    "true_mis_flat = np.concatenate(true_mis)\n",
    "print(len(loss_pressure_flat))\n",
    "print(len(true_mis_flat))\n",
    "# color = np.arange(len(loss_press))\n",
    "color = plt.cm.hsv(np.linspace(0, 1, len(true_mis_flat)))\n",
    "# color = np.arange(len(loss_pressure))\n",
    "plt.scatter(true_mis_flat, loss_pressure_flat, marker = \".\", c = color, cmap = 'hsv', s = 10)\n",
    "plt.colorbar()\n",
    "\n",
    "# for i in range(len(loss_vel_z)):\n",
    "#     plt.text(true_mis[i], loss_vel_z[i], str(i), fontsize=4, ha='right')\n",
    "\n",
    "plt.title(\"Pressure error on test sample\", size= 15)\n",
    "plt.xlabel(\"MIS\", size=15)\n",
    "plt.ylabel(\"Loss in %\", size=15)\n",
    "# plt.semilogy()\n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segnn_binarytree",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
