{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main NOTEBOOK for Dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import cos, sin, radians\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import struct\n",
    "from glob import glob\n",
    "\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import concurrent.futures\n",
    "import subprocess\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Mesh as NX graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_points_vtk(file_path):\n",
    "    points = []\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            header = []\n",
    "            for _ in range(5): # header is 5 lines\n",
    "                line = file.readline().decode('utf-8').strip()\n",
    "                header.append(line)\n",
    "            \n",
    "            if not header[0].startswith('# vtk DataFile Version'):\n",
    "                raise ValueError(\"Non è un file VTK valido\")\n",
    "            \n",
    "            points_line = header[4]\n",
    "            if not points_line.startswith('POINTS'):\n",
    "                raise ValueError(\"Formato POINTS non trovato\")\n",
    "            \n",
    "            num_points = int(points_line.split()[1])\n",
    "            \n",
    "            for _ in range(num_points):\n",
    "                point_data = file.read(12)\n",
    "                x, y, z = struct.unpack('>fff', point_data)\n",
    "                points.append((x, y, z))\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante la lettura del file: {str(e)}\")\n",
    "        return []\n",
    "        \n",
    "    return points\n",
    "\n",
    "# file_path = \"../.data/referenceCase/constant/polyMesh/points\"\n",
    "# file_path = \"../.data/referenceCase/constant/triSurface/rocket.eMesh\"\n",
    "file_path = \"../.data/referenceCase/VTK/rocket/rocket_0.VTK\"\n",
    "points = load_points_vtk(file_path)\n",
    "print(len(points))\n",
    "# print(points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_3d_graph(points, connectivity=None):\n",
    "\n",
    "    points_trace = go.Scatter3d(\n",
    "        x=[point[0] for point in points],\n",
    "        y=[point[1] for point in points],\n",
    "        z=[point[2] for point in points],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=2,\n",
    "            color='blue',\n",
    "            opacity=0.8\n",
    "        ),\n",
    "        name='Nodes'\n",
    "    )\n",
    "    \n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    edge_z = []\n",
    "    \n",
    "    if connectivity is not None:\n",
    "        for i in range(connectivity.shape[1]):\n",
    "            start_node = connectivity[0, i]\n",
    "            end_node = connectivity[1, i]\n",
    "            \n",
    "            edge_x.extend([points[start_node][0], points[end_node][0], None])\n",
    "            edge_y.extend([points[start_node][1], points[end_node][1], None])\n",
    "            edge_z.extend([points[start_node][2], points[end_node][2], None])\n",
    "        \n",
    "        edges_trace = go.Scatter3d(\n",
    "            x=edge_x,\n",
    "            y=edge_y,\n",
    "            z=edge_z,\n",
    "            mode='lines',\n",
    "            line=dict(\n",
    "                color='gray',\n",
    "                width=1\n",
    "            ),\n",
    "            opacity=0.3,\n",
    "            name='Edges'\n",
    "        )\n",
    "    \n",
    "        fig = go.Figure(data=[points_trace, edges_trace])\n",
    "    else:\n",
    "        fig = go.Figure(data=[points_trace])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title='X',\n",
    "            yaxis_title='Y',\n",
    "            zaxis_title='Z'\n",
    "        ),\n",
    "        width=1150,\n",
    "        height=800,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "plot_3d_graph(points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create nodes connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_connectivity(points, n_neighbors):\n",
    "    \n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors+1, algorithm='ball_tree')\n",
    "    nbrs.fit(points)\n",
    "    distances, indices = nbrs.kneighbors(points)\n",
    "    \n",
    "    sources = []\n",
    "    destinations = []\n",
    "    \n",
    "    for i in range(len(points)):\n",
    "        for j in indices[i][1:]:\n",
    "            sources.append(i)\n",
    "            destinations.append(j)\n",
    "            \n",
    "    connectivity = torch.tensor([sources, destinations])\n",
    "    \n",
    "    return connectivity\n",
    "\n",
    "n_neighbors = 10\n",
    "connectivity = create_connectivity(points, n_neighbors)\n",
    "\n",
    "print(connectivity)\n",
    "print(connectivity.shape)\n",
    "plot_3d_graph(points, connectivity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def ignore_vtk(dirname, filenames):\n",
    "\n",
    "#     if os.path.basename(dirname) == 'VTK':\n",
    "#         return filenames\n",
    "#     return []\n",
    "\n",
    "# def create_case_directory(base_case, case_dir):\n",
    "\n",
    "#     if os.path.exists(case_dir):\n",
    "#         shutil.rmtree(case_dir)\n",
    "    \n",
    "#     shutil.copytree(base_case, case_dir, ignore=ignore_vtk)\n",
    "\n",
    "# def modify_U_file(case_dir, velocity, angle):\n",
    "    \n",
    "#     vz = velocity * cos(radians(angle))\n",
    "#     vy = velocity * sin(radians(angle))\n",
    "    \n",
    "#     U_file = os.path.join(case_dir, \"0\", \"U\")\n",
    "#     with open(U_file, 'r') as f:\n",
    "#         lines = f.readlines()\n",
    "    \n",
    "#     new_lines = []\n",
    "#     i = 0\n",
    "#     while i < len(lines):\n",
    "#         line = lines[i]\n",
    "#         if \"internalField\" in line:\n",
    "#             new_lines.append(line)\n",
    "#             new_lines.append(f\"uniform (0 {vy} {vz});\\n\")\n",
    "#             i += 2 # ignore old line\n",
    "#         elif \"inlet\" in line and \"value\" in lines[i+2]:\n",
    "#             new_lines.extend(lines[i:i+2])\n",
    "#             new_lines.append(f\"        value           uniform (0 {vy} {vz});\\n\")\n",
    "#             i += 4 # jump to the inlet end\n",
    "#         else:\n",
    "#             new_lines.append(line)\n",
    "#             i += 1\n",
    "    \n",
    "#     with open(U_file, 'w') as f:\n",
    "#         f.writelines(new_lines)\n",
    "\n",
    "# def run_simulation(case_dir):\n",
    "\n",
    "#     abs_path = os.path.abspath(case_dir)\n",
    "#     abs_path = abs_path.replace('C:\\\\', '').replace('c:\\\\', '')\n",
    "    \n",
    "#     wsl_path = '/mnt/c/' + abs_path.replace('\\\\', '/')\n",
    "    \n",
    "#     # print(f\"Percorso WSL: {wsl_path}\")\n",
    "    \n",
    "#     wsl_command = f\"\"\"\n",
    "#     . $HOME/OpenFOAM-12/etc/bashrc && \\\n",
    "#     cd \"{wsl_path}\" && \\\n",
    "#     rhoCentralFoam\n",
    "#     \"\"\"\n",
    "    \n",
    "#     try:\n",
    "#         result = subprocess.run(\n",
    "#             ['wsl', 'bash', '-l', '-c', wsl_command],\n",
    "#             capture_output=True,\n",
    "#             text=True,\n",
    "#             check=True\n",
    "#         )\n",
    "#         print(\"Output:\", result.stdout)\n",
    "#     except subprocess.CalledProcessError as e:\n",
    "#         print(\"Runtime error:\")\n",
    "#         print(\"Output:\", e.output)\n",
    "#         print(\"Error:\", e.stderr)\n",
    "#         raise\n",
    "\n",
    "\n",
    "# n_simulations = 10\n",
    "# base_case = \"../.data/referenceCase\"\n",
    "\n",
    "# velocity_range = (20, 40)  # m/s\n",
    "# angle_range = (-5, 5)    # gradi\n",
    "\n",
    "# velocities = np.random.uniform(velocity_range[0], velocity_range[1], n_simulations)\n",
    "# angles = np.random.uniform(angle_range[0], angle_range[1], n_simulations)\n",
    "\n",
    "# simulation_parameter_dir = \"simulation_parameters.csv\"\n",
    "# with open(simulation_parameter_dir, \"w\") as f:\n",
    "#     f.write(\"case,velocity,angle\\n\")\n",
    "\n",
    "# for i in range(n_simulations):\n",
    "#     case_name = f\"case_{i:03d}\"\n",
    "#     case_dir = os.path.join(\"../.data\", case_name)\n",
    "    \n",
    "#     with open(simulation_parameter_dir, \"a\") as f:\n",
    "#         f.write(f\"{case_name},{velocities[i]:.2f},{angles[i]:.2f}\\n\")\n",
    "    \n",
    "#     print(f\"\\nRunning simulation ---> {i+1}/{n_simulations} - [velocity {velocities[i]:.2f} m/s, angle {angles[i]:.2f}]\" )\n",
    "    \n",
    "#     create_case_directory(base_case, case_dir)\n",
    "#     modify_U_file(case_dir, velocities[i], angles[i])\n",
    "    \n",
    "#     try:\n",
    "#         run_simulation(case_dir)\n",
    "#     except subprocess.CalledProcessError as e:\n",
    "#         print(f\"Error in sim {case_dir}: {e}\")\n",
    "#         continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multiprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "def ignore_vtk(dirname, filenames):\n",
    "\n",
    "    if os.path.basename(dirname) == 'VTK':\n",
    "        return filenames\n",
    "    return []\n",
    "\n",
    "def create_case_directory(base_case, case_dir):\n",
    "\n",
    "    if os.path.exists(case_dir):\n",
    "        shutil.rmtree(case_dir)\n",
    "    \n",
    "    shutil.copytree(base_case, case_dir, ignore=ignore_vtk)\n",
    "\n",
    "def modify_U_file(case_dir, velocity, angle):\n",
    "    \n",
    "    vz = velocity * cos(radians(angle))\n",
    "    vy = velocity * sin(radians(angle))\n",
    "    \n",
    "    U_file = os.path.join(case_dir, \"0\", \"U\")\n",
    "    with open(U_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    new_lines = []\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        if \"internalField\" in line:\n",
    "            new_lines.append(line)\n",
    "            new_lines.append(f\"uniform (0 {vy} {vz});\\n\")\n",
    "            i += 2 # ignore old line\n",
    "        elif \"inlet\" in line and \"value\" in lines[i+2]:\n",
    "            new_lines.extend(lines[i:i+2])\n",
    "            new_lines.append(f\"        value           uniform (0 {vy} {vz});\\n\")\n",
    "            i += 4 # jump to the inlet end\n",
    "        else:\n",
    "            new_lines.append(line)\n",
    "            i += 1\n",
    "    \n",
    "    with open(U_file, 'w') as f:\n",
    "        f.writelines(new_lines)\n",
    "\n",
    "def run_simulation(case_dir):\n",
    "\n",
    "    abs_path = os.path.abspath(case_dir)\n",
    "    abs_path = abs_path.replace('C:\\\\', '').replace('c:\\\\', '')\n",
    "    \n",
    "    wsl_path = '/mnt/c/' + abs_path.replace('\\\\', '/')\n",
    "    \n",
    "    # print(f\"WSL path: {wsl_path}\")\n",
    "    \n",
    "    wsl_command = f\"\"\"\n",
    "    . $HOME/OpenFOAM-12/etc/bashrc && \\\n",
    "    cd \"{wsl_path}\" && \\\n",
    "    rhoCentralFoam\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['wsl', 'bash', '-l', '-c', wsl_command],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        print(\"Output:\", result.stdout)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Runtime error:\")\n",
    "        print(\"Output:\", e.output)\n",
    "        print(\"Error:\", e.stderr)\n",
    "        raise\n",
    "\n",
    "def run_parallel_simulations(velocities, angles, n_simulations, base_case, simulation_parameter_dir, max_concurrent=4):\n",
    "    def run_batch(start_idx, end_idx):\n",
    "        for i in range(start_idx, end_idx):\n",
    "            case_name = f\"case_{i:03d}\"\n",
    "            case_dir = os.path.join(\"../.data\", case_name)\n",
    "            \n",
    "            print(f\"\\nRunning simulation ---> {i+1}/{n_simulations} - [velocity {velocities[i]:.2f} m/s, angle {angles[i]:.2f}]\")\n",
    "            \n",
    "            with open(simulation_parameter_dir, \"a\") as f:\n",
    "                f.write(f\"{case_name},{velocities[i]:.2f},{angles[i]:.2f}\\n\")\n",
    "            \n",
    "            create_case_directory(base_case, case_dir)\n",
    "            modify_U_file(case_dir, velocities[i], angles[i])\n",
    "            \n",
    "            try:\n",
    "                run_simulation(case_dir)\n",
    "                results[i] = True\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"Error during sim {case_dir}: {e}\")\n",
    "                results[i] = False\n",
    "\n",
    "    results = [False] * n_simulations\n",
    "    \n",
    "    batch_size = n_simulations // max_concurrent + (1 if n_simulations % max_concurrent else 0)\n",
    "    threads = []\n",
    "    \n",
    "    for i in range(0, n_simulations, batch_size):\n",
    "        end_idx = min(i + batch_size, n_simulations)\n",
    "        thread = threading.Thread(target=run_batch, args=(i, end_idx))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "    \n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    \n",
    "    return results\n",
    "    \n",
    "n_simulations = 4\n",
    "base_case = \"../.data/referenceCase\"\n",
    "\n",
    "velocity_range = (320, 360) # m/s\n",
    "angle_range = (-10, 10) # degrees\n",
    "\n",
    "velocities = np.random.uniform(velocity_range[0], velocity_range[1], n_simulations)\n",
    "angles = np.random.uniform(angle_range[0], angle_range[1], n_simulations)\n",
    "\n",
    "simulation_parameter_dir = \"simulation_parameters.csv\"\n",
    "with open(simulation_parameter_dir, \"w\") as f:\n",
    "    f.write(\"case,velocity,angle\\n\")\n",
    "\n",
    "n_workers = 4\n",
    "\n",
    "sim_args = [(velocities[i], angles[i], i, n_simulations, base_case, simulation_parameter_dir) \n",
    "            for i in range(n_simulations)]\n",
    "\n",
    "results = []\n",
    "\n",
    "with open(simulation_parameter_dir, \"w\") as f:\n",
    "    f.write(\"case,velocity,angle\\n\")\n",
    "\n",
    "results = run_parallel_simulations(velocities, angles, n_simulations, base_case, \n",
    "                                    simulation_parameter_dir, max_concurrent=4)\n",
    "\n",
    "successful = sum(results)\n",
    "print(f\"\\nCompleted simulations: {successful}/{n_simulations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_data(data, method='minmax'):\n",
    "\n",
    "#     if method == 'minmax':\n",
    "#         data_min = np.min(data)\n",
    "#         data_max = np.max(data)\n",
    "#         if data_max - data_min != 0:\n",
    "#             normalized = (data - data_min) / (data_max - data_min)\n",
    "#         else:\n",
    "#             normalized = data\n",
    "#         params = {'min': data_min, 'max': data_max}\n",
    "        \n",
    "#     elif method == 'standard':\n",
    "#         mean = np.mean(data)\n",
    "#         std = np.std(data)\n",
    "#         if std != 0:\n",
    "#             normalized = (data - mean) / std\n",
    "#         else:\n",
    "#             normalized = data\n",
    "#         params = {'mean': mean, 'std': std}\n",
    "        \n",
    "#     return normalized, params\n",
    "\n",
    "# def normalize_data_global(data, field_name, global_mins, global_maxs):\n",
    "\n",
    "#     data_min = global_mins[field_name]\n",
    "#     data_max = global_maxs[field_name]\n",
    "#     if data_max - data_min != 0:\n",
    "#         normalized = (data - data_min) / (data_max - data_min)\n",
    "#     else:\n",
    "#         normalized = data\n",
    "        \n",
    "#     return normalized, {'min': data_min, 'max': data_max}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data from simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_matching_indices(mesh_points, surface_points, field_size, tolerance=1e-6):\n",
    "\n",
    "    indices = []\n",
    "    mesh_points = np.array(mesh_points)[:field_size]\n",
    "    surface_points = np.array(surface_points)\n",
    "    \n",
    "    print(f\"Mesh points shape: {mesh_points.shape}\")\n",
    "    print(f\"Surface points shape: {surface_points.shape}\")\n",
    "    \n",
    "    for i, surface_point in enumerate(surface_points):\n",
    "        distances = np.linalg.norm(mesh_points - surface_point, axis=1)\n",
    "        matching = np.where(distances < tolerance)[0]\n",
    "\n",
    "        if len(matching) > 0:\n",
    "            indices.append(matching[0])\n",
    "        else:\n",
    "            closest_idx = np.argmin(distances)\n",
    "            min_distance = distances[closest_idx]\n",
    "            # print(f\"Warning: No matching point found for surface point {i}, using closest at distance {min_distance}\")\n",
    "            indices.append(closest_idx)\n",
    "    \n",
    "    indices = np.array(indices)\n",
    "    # print(f\"Found {len(indices)} matching points\")\n",
    "    return indices\n",
    "\n",
    "def get_latest_time(case_dir):\n",
    "\n",
    "    time_dirs = [d for d in os.listdir(case_dir) \n",
    "                if d.replace('.','').isdigit() and os.path.isdir(os.path.join(case_dir, d))]\n",
    "    if not time_dirs:\n",
    "        return None\n",
    "    \n",
    "    latest_time = max(float(t) for t in time_dirs)\n",
    "    if latest_time == 1.0:\n",
    "        latest_time = int(latest_time)\n",
    "        \n",
    "    return str(latest_time)\n",
    "\n",
    "def read_openfoam_points(filename):\n",
    "\n",
    "    points = []\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        start_idx = 0\n",
    "        for i, line in enumerate(lines):\n",
    "            if line.strip() == '(':\n",
    "                start_idx = i + 1\n",
    "                break\n",
    "        \n",
    "        for line in lines[start_idx:]:\n",
    "            line = line.strip()\n",
    "            if line == ')':\n",
    "                break\n",
    "            \n",
    "            if line.startswith('(') and line.endswith(')'):\n",
    "                values = line.strip('()').split()\n",
    "                values = [float(x) for x in values]\n",
    "                points.append(values)\n",
    "    \n",
    "    return np.array(points)\n",
    "\n",
    "def read_openfoam_field(filename):\n",
    "\n",
    "    data = []\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        start_idx = 0\n",
    "        for i, line in enumerate(lines):\n",
    "            if line.strip() == '(':\n",
    "                start_idx = i + 1\n",
    "                break\n",
    "        \n",
    "        for line in lines[start_idx:]:\n",
    "            line = line.strip()\n",
    "            if line == ')':\n",
    "                break\n",
    "            \n",
    "            if line.startswith('(') and line.endswith(')'):\n",
    "\n",
    "                values = line.strip('()').split()\n",
    "\n",
    "                values = [float(x) for x in values]\n",
    "                data.append(values)\n",
    "\n",
    "            elif line:\n",
    "                try:\n",
    "                    value = float(line)\n",
    "                    data.append(value)\n",
    "                except ValueError as e: # for some reason it often breaks here\n",
    "                    print(f\"Parsing error at line: {line}\")\n",
    "                    raise e\n",
    "    \n",
    "    return np.array(data)\n",
    "\n",
    "def extract_case_data(case_dir, points):\n",
    "    latest_time = get_latest_time(case_dir)\n",
    "    # print(latest_time)\n",
    "    if latest_time is None:\n",
    "        print(f\"No data in {case_dir}\")\n",
    "        return None\n",
    "\n",
    "    time_dir = os.path.join(case_dir, latest_time)\n",
    "    data = {}\n",
    "\n",
    "    mesh_points_file = os.path.join(case_dir, 'constant/polyMesh/points')\n",
    "    if os.path.exists(mesh_points_file):\n",
    "\n",
    "        mesh_points = read_openfoam_points(mesh_points_file)\n",
    "        print(f\"Total mesh points: {len(mesh_points)}\")\n",
    "        surface_indices = find_matching_indices(mesh_points, points, field_size=len(mesh_points))\n",
    "\n",
    "    else:\n",
    "        print(f\"Mesh points file not found in {case_dir}\")\n",
    "        return None\n",
    "\n",
    "    fields = {\n",
    "        'p': 'p',\n",
    "        'U': 'U',\n",
    "        'T': 'T',\n",
    "        'rho': 'rho',\n",
    "        'phi': 'phi'\n",
    "    }\n",
    "\n",
    "    fields_data = {}\n",
    "    for field_name, file_name in fields.items():\n",
    "\n",
    "        file_path = os.path.join(time_dir, file_name)\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"{field_name} not found in {case_dir}\")\n",
    "            return None\n",
    "        \n",
    "        fields_data[field_name] = read_openfoam_field(file_path)\n",
    "        print(f\"Field {field_name} shape: {fields_data[field_name].shape}\")\n",
    "\n",
    "    min_field_size = min(len(field) for field in fields_data.values())\n",
    "    print(f\"Minimum field size: {min_field_size}\")\n",
    "\n",
    "    surface_indices = find_matching_indices(mesh_points, points, min_field_size)\n",
    "\n",
    "    data = {}\n",
    "    for field_name in fields:\n",
    "\n",
    "        data[field_name] = fields_data[field_name][surface_indices]\n",
    "        print(f\"Extracted {field_name} values: {len(data[field_name])}\")\n",
    "\n",
    "        assert len(data[field_name]) == len(points)\n",
    "\n",
    "    return data\n",
    "\n",
    "### DIR SETTINGS\n",
    "\n",
    "base_dir = '.'\n",
    "output_dir = '../.data/Extracted_data'\n",
    "output_dir_probes = output_dir + '/probes'\n",
    "output_dir_features = output_dir + '/fields'\n",
    "checkpoints_dir_features = output_dir + '/checkpoints'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(output_dir_probes, exist_ok=True)\n",
    "os.makedirs(output_dir_features, exist_ok=True)\n",
    "os.makedirs(checkpoints_dir_features, exist_ok=True)\n",
    "\n",
    "simulation_params_file = 'simulation_parameters.csv'\n",
    "shutil.copy(simulation_params_file, output_dir)\n",
    "\n",
    "simulation_params = pd.read_csv(simulation_params_file)\n",
    "case_params = {row['case']: (row['velocity'], row['angle']) for _, row in simulation_params.iterrows()}\n",
    "print(f\"case_params: {case_params}\")\n",
    "\n",
    "case_dirs = sorted(glob('../.data/case_[0-9][0-9][0-9]'))\n",
    "print(f\"case_dirs: {case_dirs}\")\n",
    "\n",
    "### EXTRACTAD DATA\n",
    "\n",
    "for case_dir in case_dirs:\n",
    "    case_num = case_dir[-3:]\n",
    "    print(f\"Processing {case_dir}...\")\n",
    "\n",
    "    data = extract_case_data(case_dir, points)\n",
    "    if data is None:\n",
    "        continue\n",
    "\n",
    "    # simulated features\n",
    "    p = data['p']\n",
    "    U = data['U']\n",
    "    T = data['T']\n",
    "    rho = data['rho']\n",
    "    phi = data['phi']\n",
    "    # print(p)\n",
    "    \n",
    "    zero = np.zeros((len(points)))\n",
    "    zeros = np.zeros((len(points), 3))\n",
    "\n",
    "    # get velocity and angle from the dict\n",
    "    case_key = f\"case_{case_num}\"\n",
    "    if case_key in case_params:\n",
    "        velocity, angle = case_params[case_key]\n",
    "    else:\n",
    "        print(f\"Warning: Parameters not found for {case_key}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # print(len(points))\n",
    "    # print(points)\n",
    "    velocity_array = np.full(len(points), velocity).tolist()\n",
    "    # print(velocity_array)\n",
    "    # print(len(velocity_array))\n",
    "    angle_array = np.full(len(points), angle).tolist()\n",
    "    # print(pressure_array)\n",
    "    # print(len(pressure_array))\n",
    "    # print(len(zero))\n",
    "\n",
    "    velocity_column = np.array(velocity_array).reshape(-1, 1)\n",
    "    angle_column = np.array(angle_array).reshape(-1, 1)\n",
    "    angle_column = np.concatenate(angle_column)\n",
    "    # print(angle_column)\n",
    "    zero_column = zero.reshape(-1, 1)\n",
    "\n",
    "    output_file = os.path.join(output_dir_features, f'SIM{case_num}_features.npz')\n",
    "    np.savez(output_file,\n",
    "             p=p,\n",
    "             U_x=U[:, 0],\n",
    "             U_y=U[:, 1],\n",
    "             U_z=U[:, 2],\n",
    "             T=T,\n",
    "             rho=rho,\n",
    "             phi=phi,\n",
    "             real_velocity_0=velocity_array,\n",
    "             real_angle_0=angle_column,\n",
    "             zero3=zero) # <-------------- TEMPORARY: i need 10 columns to match save.ipynb \n",
    "    \n",
    "    points_output_file = os.path.join(output_dir_probes, f'SIM{case_num}_probepos.npy')\n",
    "    points_with_zeros = np.hstack((points, zeros))\n",
    "    np.save(points_output_file, points_with_zeros) # <-------------- TEMPORARY: i need 6 columns to match save.ipynb\n",
    "\n",
    "\n",
    "# save previously computed connectivity\n",
    "np.save('../.data/Extracted_data/connectivity.npy', connectivity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def find_matching_indices(mesh_points, surface_points, field_size, tolerance=1e-6):\n",
    "#     \"\"\"\n",
    "#     field_size: dimensione del campo più piccolo (p, U, T, etc)\n",
    "#     \"\"\"\n",
    "#     indices = []\n",
    "#     mesh_points = np.array(mesh_points)[:field_size]\n",
    "#     surface_points = np.array(surface_points)\n",
    "    \n",
    "#     print(f\"Mesh points shape: {mesh_points.shape}\")\n",
    "#     print(f\"Surface points shape: {surface_points.shape}\")\n",
    "    \n",
    "#     for i, surface_point in enumerate(surface_points):\n",
    "#         distances = np.linalg.norm(mesh_points - surface_point, axis=1)\n",
    "#         matching = np.where(distances < tolerance)[0]\n",
    "\n",
    "#         if len(matching) > 0:\n",
    "#             indices.append(matching[0])\n",
    "#         else:\n",
    "#             closest_idx = np.argmin(distances)\n",
    "#             min_distance = distances[closest_idx]\n",
    "#             # print(f\"Warning: No matching point found for surface point {i}, using closest at distance {min_distance}\")\n",
    "#             indices.append(closest_idx)\n",
    "    \n",
    "#     indices = np.array(indices)\n",
    "#     # print(f\"Found {len(indices)} matching points\")\n",
    "#     return indices\n",
    "\n",
    "# def get_latest_time(case_dir):\n",
    "\n",
    "#     time_dirs = [d for d in os.listdir(case_dir) \n",
    "#                 if d.replace('.','').isdigit() and os.path.isdir(os.path.join(case_dir, d))]\n",
    "#     if not time_dirs:\n",
    "#         return None\n",
    "    \n",
    "#     latest_time = max(float(t) for t in time_dirs)\n",
    "#     if latest_time == 1.0:\n",
    "#         latest_time = int(latest_time)\n",
    "#     return str(latest_time)\n",
    "\n",
    "# def read_openfoam_points(filename):\n",
    "\n",
    "#     points = []\n",
    "#     with open(filename, 'r') as f:\n",
    "#         lines = f.readlines()\n",
    "#         start_idx = 0\n",
    "#         for i, line in enumerate(lines):\n",
    "#             if line.strip() == '(':\n",
    "#                 start_idx = i + 1\n",
    "#                 break\n",
    "        \n",
    "#         for line in lines[start_idx:]:\n",
    "#             line = line.strip()\n",
    "#             if line == ')':\n",
    "#                 break\n",
    "            \n",
    "#             if line.startswith('(') and line.endswith(')'):\n",
    "#                 values = line.strip('()').split()\n",
    "#                 values = [float(x) for x in values]\n",
    "#                 points.append(values)\n",
    "    \n",
    "#     return np.array(points)\n",
    "\n",
    "# def read_openfoam_field(filename):\n",
    "\n",
    "#     data = []\n",
    "#     with open(filename, 'r') as f:\n",
    "#         lines = f.readlines()\n",
    "#         start_idx = 0\n",
    "#         for i, line in enumerate(lines):\n",
    "#             if line.strip() == '(':\n",
    "#                 start_idx = i + 1\n",
    "#                 break\n",
    "        \n",
    "#         for line in lines[start_idx:]:\n",
    "#             line = line.strip()\n",
    "#             if line == ')':\n",
    "#                 break\n",
    "            \n",
    "#             if line.startswith('(') and line.endswith(')'):\n",
    "\n",
    "#                 values = line.strip('()').split()\n",
    "\n",
    "#                 values = [float(x) for x in values]\n",
    "#                 data.append(values)\n",
    "\n",
    "#             elif line:\n",
    "#                 try:\n",
    "#                     value = float(line)\n",
    "#                     data.append(value)\n",
    "#                 except ValueError as e: # for some reason it often breaks here\n",
    "#                     print(f\"Parsing error at line: {line}\")\n",
    "#                     raise e\n",
    "    \n",
    "#     return np.array(data)\n",
    "\n",
    "# def extract_case_data(case_dir, points):\n",
    "#     latest_time = get_latest_time(case_dir)\n",
    "#     # print(latest_time)\n",
    "#     if latest_time is None:\n",
    "#         print(f\"No data in {case_dir}\")\n",
    "#         return None\n",
    "\n",
    "#     time_dir = os.path.join(case_dir, latest_time)\n",
    "#     data = {}\n",
    "\n",
    "#     mesh_points_file = os.path.join(case_dir, 'constant/polyMesh/points')\n",
    "#     if os.path.exists(mesh_points_file):\n",
    "\n",
    "#         mesh_points = read_openfoam_points(mesh_points_file)\n",
    "#         print(f\"Total mesh points: {len(mesh_points)}\")\n",
    "#         surface_indices = find_matching_indices(mesh_points, points, field_size=len(mesh_points))\n",
    "\n",
    "#     else:\n",
    "#         print(f\"Mesh points file not found in {case_dir}\")\n",
    "#         return None\n",
    "\n",
    "#     fields = {\n",
    "#         'p': 'p',\n",
    "#         'U': 'U',\n",
    "#         'T': 'T',\n",
    "#         'rho': 'rho',\n",
    "#         'phi': 'phi'\n",
    "#     }\n",
    "\n",
    "#     fields_data = {}\n",
    "#     for field_name, file_name in fields.items():\n",
    "#         file_path = os.path.join(time_dir, file_name)\n",
    "#         if not os.path.exists(file_path):\n",
    "#             print(f\"{field_name} not found in {case_dir}\")\n",
    "#             return None\n",
    "#         fields_data[field_name] = read_openfoam_field(file_path)\n",
    "#         print(f\"Field {field_name} shape: {fields_data[field_name].shape}\")\n",
    "\n",
    "#     min_field_size = min(len(field) for field in fields_data.values())\n",
    "#     print(f\"Minimum field size: {min_field_size}\")\n",
    "\n",
    "#     surface_indices = find_matching_indices(mesh_points, points, min_field_size)\n",
    "\n",
    "#     data = {}\n",
    "#     for field_name in fields:\n",
    "#         data[field_name] = fields_data[field_name][surface_indices]\n",
    "#         print(f\"Extracted {field_name} values: {len(data[field_name])}\")\n",
    "\n",
    "#         assert len(data[field_name]) == len(points)\n",
    "\n",
    "#     return data\n",
    "\n",
    "# base_dir = '.'\n",
    "# output_dir_probes = '../.data/Extracted_data/probes'\n",
    "# output_dir_features = '../.data/Extracted_data/fields'\n",
    "# checkpoints_dir_features = '../.data/Extracted_data/checkpoints'\n",
    "# os.makedirs(output_dir_probes, exist_ok=True)\n",
    "# os.makedirs(output_dir_features, exist_ok=True)\n",
    "# os.makedirs(checkpoints_dir_features, exist_ok=True)\n",
    "\n",
    "# case_dirs = sorted(glob('../.data/case_[0-9][0-9][0-9]'))\n",
    "\n",
    "\n",
    "# ### FIND GLOBAL MAX AND MIN\n",
    "\n",
    "# global_mins = {}\n",
    "# global_maxs = {}\n",
    "\n",
    "# for case_dir in case_dirs:\n",
    "#     case_num = case_dir[-3:]\n",
    "#     print(f\"Finding global min/max in {case_dir}...\")\n",
    "    \n",
    "#     data = extract_case_data(case_dir, points)\n",
    "#     if data is None:\n",
    "#         continue\n",
    "        \n",
    "#     fields_to_check = {\n",
    "#         'p': data['p'],\n",
    "#         'U_x': data['U'][:, 0],\n",
    "#         'U_y': data['U'][:, 1],\n",
    "#         'U_z': data['U'][:, 2],\n",
    "#         'T': data['T'],\n",
    "#         'rho': data['rho'],\n",
    "#         'phi': data['phi']\n",
    "#     }\n",
    "    \n",
    "#     for field_name, field_data in fields_to_check.items():\n",
    "#         if field_name not in global_mins:\n",
    "#             global_mins[field_name] = np.min(field_data)\n",
    "#             global_maxs[field_name] = np.max(field_data)\n",
    "#         else:\n",
    "#             global_mins[field_name] = min(global_mins[field_name], np.min(field_data))\n",
    "#             global_maxs[field_name] = max(global_maxs[field_name], np.max(field_data))\n",
    "\n",
    "\n",
    "# ### SAVE DATA\n",
    "\n",
    "# for case_dir in case_dirs:\n",
    "#     case_num = case_dir[-3:]\n",
    "#     print(f\"Processing {case_dir}...\")\n",
    "    \n",
    "#     data = extract_case_data(case_dir, points)\n",
    "#     if data is None:\n",
    "#         continue\n",
    "        \n",
    "#     normalized_data = {}\n",
    "#     normalization_params = {}\n",
    "    \n",
    "#     fields_to_normalize = {\n",
    "#         'p': data['p'],\n",
    "#         'U_x': data['U'][:, 0],\n",
    "#         'U_y': data['U'][:, 1],\n",
    "#         'U_z': data['U'][:, 2],\n",
    "#         'T': data['T'],\n",
    "#         'rho': data['rho'],\n",
    "#         'phi': data['phi']\n",
    "#     }\n",
    "    \n",
    "#     for field_name, field_data in fields_to_normalize.items():\n",
    "#         normalized_data[field_name], normalization_params[field_name] = normalize_data_global(field_data, field_name, global_mins, global_maxs)\n",
    "\n",
    "#     zero = np.zeros((len(points)))\n",
    "#     zeros = np.zeros((len(points), 3))\n",
    "\n",
    "#     output_file = os.path.join(output_dir_features, f'SIM{case_num}_features.npz')\n",
    "#     np.savez(output_file,\n",
    "#             p=normalized_data['p'],\n",
    "#             U_x=normalized_data['U_x'],\n",
    "#             U_y=normalized_data['U_y'],\n",
    "#             U_z=normalized_data['U_z'],\n",
    "#             T=normalized_data['T'],\n",
    "#             rho=normalized_data['rho'],\n",
    "#             phi=normalized_data['phi'],\n",
    "#             zero1=zero, # <--------------\n",
    "#             zero2=zero, # <-------------- TEMPORARY: i need 10 columns to match save.ipynb\n",
    "#             zero3=zero) # <--------------\n",
    "    \n",
    "#     points_output_file = os.path.join(output_dir_probes, f'SIM{case_num}_probepos.npy')\n",
    "#     points_with_zeros = np.hstack((points, zeros))\n",
    "#     np.save(points_output_file, points_with_zeros) # <-------------- TEMPORARY: i need 6 columns to match save.ipynb\n",
    "\n",
    "\n",
    "# # save previously computed connectivity\n",
    "# np.save('../.data/Extracted_data/connectivity.npy', connectivity)\n",
    "\n",
    "\n",
    "# # also save global params for normalization\n",
    "# global_params = {\n",
    "#     'mins': global_mins,\n",
    "#     'maxs': global_maxs\n",
    "# }\n",
    "# np.savez('../.data/Extracted_data/global_norm_params.npz', **global_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_test_18",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
